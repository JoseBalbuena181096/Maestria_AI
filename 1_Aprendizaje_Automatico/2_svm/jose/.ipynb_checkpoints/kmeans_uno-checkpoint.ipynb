{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "header"
   },
   "source": [
    "# Clasificación de Iris con K-Nearest Neighbors (K-NN)\n",
    "\n",
    "## Introducción\n",
    "\n",
    "Este notebook presenta un análisis completo de clasificación multiclase utilizando el algoritmo K-Nearest Neighbors (K-NN) sobre el conjunto de datos Iris. El objetivo es clasificar tres especies de flores Iris basándose únicamente en dos características: longitud y ancho del sépalo.\n",
    "\n",
    "### Objetivos\n",
    "- Implementar un pipeline de Machine Learning con preprocesamiento y clasificación\n",
    "- Comparar el rendimiento de K-NN con diferentes estrategias de ponderación\n",
    "- Visualizar las fronteras de decisión del clasificador\n",
    "- Evaluar el desempeño del modelo mediante métricas estándar\n",
    "\n",
    "**Autor:** Balbuena Palma José Ángel\n",
    "\n",
    "**Fecha:** 25 Octubre 2025"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "imports"
   },
   "source": [
    "## 1. Importación de Librerías"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "import_libs"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ERROR! Session/line number was not unique in database. History logging moved to new session 145\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.inspection import DecisionBoundaryDisplay\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "\n",
    "# Configuración de estilo para gráficas\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "sns.set_palette(\"husl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "data_loading"
   },
   "source": [
    "## 2. Carga y Exploración de Datos\n",
    "\n",
    "El conjunto de datos Iris contiene 150 muestras de tres especies de flores Iris:\n",
    "- Iris Setosa\n",
    "- Iris Versicolor\n",
    "- Iris Virginica\n",
    "\n",
    "Para este análisis, utilizaremos únicamente dos características:\n",
    "- Longitud del sépalo (sepal length)\n",
    "- Ancho del sépalo (sepal width)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "load_data"
   },
   "outputs": [],
   "source": [
    "# Cargar dataset\n",
    "iris = load_iris(as_frame=True)\n",
    "X = iris.data[[\"sepal length (cm)\", \"sepal width (cm)\"]]\n",
    "y = iris.target\n",
    "\n",
    "print(\"Dimensiones del dataset:\")\n",
    "print(f\"Características (X): {X.shape}\")\n",
    "print(f\"Etiquetas (y): {y.shape}\")\n",
    "print(f\"\\nClases: {iris.target_names}\")\n",
    "print(f\"\\nDistribución de clases:\\n{y.value_counts().sort_index()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "viz_exploratory"
   },
   "outputs": [],
   "source": [
    "# Visualización exploratoria\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Scatter plot\n",
    "for target, name in enumerate(iris.target_names):\n",
    "    mask = y == target\n",
    "    axes[0].scatter(X.loc[mask, \"sepal length (cm)\"], \n",
    "                    X.loc[mask, \"sepal width (cm)\"], \n",
    "                    label=name, \n",
    "                    alpha=0.7, \n",
    "                    edgecolors='k',\n",
    "                    s=80)\n",
    "\n",
    "axes[0].set_xlabel(\"Longitud del sépalo (cm)\", fontsize=11)\n",
    "axes[0].set_ylabel(\"Ancho del sépalo (cm)\", fontsize=11)\n",
    "axes[0].set_title(\"Distribución de clases en el espacio de características\", fontsize=12, fontweight='bold')\n",
    "axes[0].legend(title=\"Especies\")\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Box plots\n",
    "X_with_target = X.copy()\n",
    "X_with_target['species'] = y.map({i: name for i, name in enumerate(iris.target_names)})\n",
    "X_melted = X_with_target.melt(id_vars='species', var_name='feature', value_name='value')\n",
    "\n",
    "sns.boxplot(data=X_melted, x='feature', y='value', hue='species', ax=axes[1])\n",
    "axes[1].set_title(\"Distribución de características por especie\", fontsize=12, fontweight='bold')\n",
    "axes[1].set_xlabel(\"Característica\", fontsize=11)\n",
    "axes[1].set_ylabel(\"Valor (cm)\", fontsize=11)\n",
    "axes[1].legend(title=\"Especies\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "statistics"
   },
   "outputs": [],
   "source": [
    "# Estadísticas descriptivas\n",
    "print(\"Estadísticas descriptivas por clase:\\n\")\n",
    "for target, name in enumerate(iris.target_names):\n",
    "    print(f\"\\n{name}:\")\n",
    "    print(X[y == target].describe().round(2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "data_split"
   },
   "source": [
    "## 3. División de Datos\n",
    "\n",
    "Dividimos el dataset en conjuntos de entrenamiento (75%) y prueba (25%) utilizando estratificación para mantener la proporción de clases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "split_data"
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, \n",
    "    test_size=0.25,\n",
    "    stratify=y, \n",
    "    random_state=0\n",
    ")\n",
    "\n",
    "print(f\"Tamaño conjunto de entrenamiento: {X_train.shape[0]} muestras\")\n",
    "print(f\"Tamaño conjunto de prueba: {X_test.shape[0]} muestras\")\n",
    "print(f\"\\nDistribución en entrenamiento:\\n{y_train.value_counts().sort_index()}\")\n",
    "print(f\"\\nDistribución en prueba:\\n{y_test.value_counts().sort_index()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pipeline"
   },
   "source": [
    "## 4. Construcción del Pipeline\n",
    "\n",
    "Creamos un pipeline que incluye:\n",
    "1. **StandardScaler**: Normaliza las características (media=0, desviación estándar=1)\n",
    "2. **KNeighborsClassifier**: Clasificador K-NN con k=11 vecinos\n",
    "\n",
    "### ¿Por qué normalizar?\n",
    "K-NN es sensible a la escala de las características ya que utiliza distancias. La normalización asegura que todas las características contribuyan equitativamente al cálculo de distancia."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "create_pipeline"
   },
   "outputs": [],
   "source": [
    "# Crear pipeline\n",
    "clf = Pipeline(\n",
    "    steps=[\n",
    "        (\"scaler\", StandardScaler()), \n",
    "        (\"knn\", KNeighborsClassifier(n_neighbors=11))\n",
    "    ]\n",
    ")\n",
    "\n",
    "print(\"Pipeline configurado:\")\n",
    "print(clf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "training"
   },
   "source": [
    "## 5. Entrenamiento y Comparación de Modelos\n",
    "\n",
    "Entrenaremos dos variantes del modelo K-NN:\n",
    "\n",
    "### 5.1 Ponderación Uniforme (`uniform`)\n",
    "Todos los vecinos tienen el mismo peso en la votación, independientemente de su distancia.\n",
    "\n",
    "### 5.2 Ponderación por Distancia (`distance`)\n",
    "Los vecinos más cercanos tienen mayor peso en la decisión. El peso es inversamente proporcional a la distancia."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "train_models"
   },
   "outputs": [],
   "source": [
    "# Diccionario para almacenar resultados\n",
    "results = {}\n",
    "\n",
    "for weights in [\"uniform\", \"distance\"]:\n",
    "    # Entrenar modelo\n",
    "    clf.set_params(knn__weights=weights).fit(X_train, y_train)\n",
    "    \n",
    "    # Predicciones\n",
    "    y_pred = clf.predict(X_test)\n",
    "    \n",
    "    # Calcular métricas\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    \n",
    "    # Almacenar resultados\n",
    "    results[weights] = {\n",
    "        'accuracy': accuracy,\n",
    "        'predictions': y_pred,\n",
    "        'model': clf\n",
    "    }\n",
    "    \n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Resultados con weights='{weights}'\")\n",
    "    print(f\"{'='*60}\")\n",
    "    print(f\"Accuracy: {accuracy:.4f} ({accuracy*100:.2f}%)\")\n",
    "    print(f\"\\nReporte de clasificación:\")\n",
    "    print(classification_report(y_test, y_pred, target_names=iris.target_names))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "confusion_matrix"
   },
   "source": [
    "## 6. Matrices de Confusión"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "plot_confusion"
   },
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "for ax, weights in zip(axes, [\"uniform\", \"distance\"]):\n",
    "    cm = confusion_matrix(y_test, results[weights]['predictions'])\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', ax=ax,\n",
    "                xticklabels=iris.target_names, \n",
    "                yticklabels=iris.target_names,\n",
    "                cbar_kws={'label': 'Número de muestras'})\n",
    "    ax.set_title(f'Matriz de Confusión\\n(weights={weights!r})', \n",
    "                 fontsize=12, fontweight='bold')\n",
    "    ax.set_ylabel('Clase Real', fontsize=11)\n",
    "    ax.set_xlabel('Clase Predicha', fontsize=11)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "decision_boundary"
   },
   "source": [
    "## 7. Visualización de Fronteras de Decisión\n",
    "\n",
    "Las fronteras de decisión muestran cómo el clasificador divide el espacio de características en regiones correspondientes a cada clase."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "plot_boundaries"
   },
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(ncols=2, figsize=(15, 6))\n",
    "\n",
    "for ax, weights in zip(axs, (\"uniform\", \"distance\")):\n",
    "    # Entrenar modelo\n",
    "    clf.set_params(knn__weights=weights).fit(X_train, y_train)\n",
    "    \n",
    "    # Crear visualización de fronteras\n",
    "    disp = DecisionBoundaryDisplay.from_estimator(\n",
    "        clf,\n",
    "        X_test,\n",
    "        response_method=\"predict\",\n",
    "        plot_method=\"pcolormesh\",\n",
    "        xlabel=iris.feature_names[0],\n",
    "        ylabel=iris.feature_names[1],\n",
    "        shading=\"auto\",\n",
    "        alpha=0.5,\n",
    "        ax=ax,\n",
    "    )\n",
    "    \n",
    "    # Agregar puntos de datos\n",
    "    scatter = disp.ax_.scatter(\n",
    "        X.iloc[:, 0], \n",
    "        X.iloc[:, 1], \n",
    "        c=y, \n",
    "        edgecolors=\"k\",\n",
    "        s=60,\n",
    "        alpha=0.8\n",
    "    )\n",
    "    \n",
    "    # Configurar leyenda\n",
    "    disp.ax_.legend(\n",
    "        scatter.legend_elements()[0],\n",
    "        iris.target_names,\n",
    "        loc=\"lower left\",\n",
    "        title=\"Especies\",\n",
    "        framealpha=0.9\n",
    "    )\n",
    "    \n",
    "    # Título con accuracy\n",
    "    acc = results[weights]['accuracy']\n",
    "    _ = disp.ax_.set_title(\n",
    "        f\"Clasificación 3-Class\\n(k={clf[-1].n_neighbors}, weights={weights!r}, accuracy={acc:.3f})\",\n",
    "        fontsize=12,\n",
    "        fontweight='bold'\n",
    "    )\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sensitivity"
   },
   "source": [
    "## 8. Análisis de Sensibilidad: Número de Vecinos (k)\n",
    "\n",
    "Exploramos cómo varía el rendimiento del modelo con diferentes valores de k."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sensitivity_analysis"
   },
   "outputs": [],
   "source": [
    "# Rango de k valores a probar\n",
    "k_values = range(1, 31)\n",
    "accuracies_uniform = []\n",
    "accuracies_distance = []\n",
    "\n",
    "for k in k_values:\n",
    "    for weights in [\"uniform\", \"distance\"]:\n",
    "        clf_temp = Pipeline([\n",
    "            (\"scaler\", StandardScaler()), \n",
    "            (\"knn\", KNeighborsClassifier(n_neighbors=k, weights=weights))\n",
    "        ])\n",
    "        clf_temp.fit(X_train, y_train)\n",
    "        acc = clf_temp.score(X_test, y_test)\n",
    "        \n",
    "        if weights == \"uniform\":\n",
    "            accuracies_uniform.append(acc)\n",
    "        else:\n",
    "            accuracies_distance.append(acc)\n",
    "\n",
    "# Visualización\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(k_values, accuracies_uniform, marker='o', label='Uniform', linewidth=2)\n",
    "plt.plot(k_values, accuracies_distance, marker='s', label='Distance', linewidth=2)\n",
    "plt.xlabel('Número de vecinos (k)', fontsize=12)\n",
    "plt.ylabel('Accuracy', fontsize=12)\n",
    "plt.title('Impacto del número de vecinos en el rendimiento del modelo', \n",
    "          fontsize=13, fontweight='bold')\n",
    "plt.legend(title='Ponderación')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.xticks(range(1, 31, 2))\n",
    "plt.axvline(x=11, color='red', linestyle='--', alpha=0.5, label='k=11 (usado)')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Mejor k para cada método\n",
    "best_k_uniform = k_values[np.argmax(accuracies_uniform)]\n",
    "best_k_distance = k_values[np.argmax(accuracies_distance)]\n",
    "\n",
    "print(f\"Mejor k para 'uniform': {best_k_uniform} (accuracy: {max(accuracies_uniform):.4f})\")\n",
    "print(f\"Mejor k para 'distance': {best_k_distance} (accuracy: {max(accuracies_distance):.4f})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "conclusions"
   },
   "source": [
    "## 9. Conclusiones\n",
    "\n",
    "### Hallazgos Principales:\n",
    "\n",
    "1. **Rendimiento del modelo**: Ambas estrategias de ponderación (uniform y distance) muestran un buen desempeño en la clasificación de especies de Iris usando solo dos características.\n",
    "\n",
    "2. **Comparación de ponderaciones**:\n",
    "   - **Uniform**: Trata a todos los vecinos por igual\n",
    "   - **Distance**: Da más peso a vecinos cercanos, lo que puede ser beneficioso cuando hay variabilidad en la densidad de datos\n",
    "\n",
    "3. **Frontera de decisión**: Las visualizaciones muestran cómo el algoritmo divide el espacio de características. La ponderación por distancia tiende a crear fronteras más suaves.\n",
    "\n",
    "4. **Número de vecinos (k)**:\n",
    "   - Valores pequeños de k: Mayor sensibilidad al ruido, posible sobreajuste\n",
    "   - Valores grandes de k: Fronteras de decisión más suaves, posible subajuste\n",
    "   - k=11 proporciona un buen balance para este dataset\n",
    "\n",
    "5. **Limitaciones**:\n",
    "   - Solo se utilizaron 2 de las 4 características disponibles\n",
    "   - Algunas clases pueden solaparse en el espacio reducido\n",
    "\n",
    "### Recomendaciones:\n",
    "- Para este dataset específico, ambas estrategias funcionan bien\n",
    "- La elección entre uniform y distance dependerá del problema específico\n",
    "- Se recomienda validación cruzada para una evaluación más robusta\n",
    "- Considerar incluir las 4 características para mejorar la separabilidad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "references"
   },
   "source": [
    "## 10. Referencias y Recursos\n",
    "\n",
    "- **Scikit-learn Documentation**: https://scikit-learn.org/\n",
    "- **Iris Dataset**: Fisher, R.A. \"The use of multiple measurements in taxonomic problems\" (1936)\n",
    "- **K-NN Algorithm**: Cover, T., Hart, P. \"Nearest neighbor pattern classification\" (1967)\n",
    "\n",
    "---\n",
    "\n",
    "**Autor**: Análisis de K-NN sobre Iris Dataset  \n",
    "**Fecha**: 2025  \n",
    "**Herramientas**: Python, Scikit-learn, Matplotlib, Seaborn"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "KNN_Iris_Classification_Report.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
