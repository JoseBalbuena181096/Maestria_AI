{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clasificación con Support Vector Machine (SVM)\n",
    "\n",
    "**Autor:** Balbuena Palma José Ángel\n",
    "\n",
    "**Fecha:** 25 Octubre 2025\n",
    "\n",
    "## Introducción\n",
    "\n",
    "En este notebook exploraremos el funcionamiento de las **Máquinas de Vectores de Soporte (SVM)** con kernel lineal para problemas de clasificación binaria. Las SVM son algoritmos de aprendizaje supervisado que buscan encontrar el hiperplano óptimo que mejor separa las clases en el espacio de características.\n",
    "\n",
    "### Objetivos\n",
    "- Generar un conjunto de datos sintético linealmente separable\n",
    "- Entrenar un clasificador SVM con kernel lineal\n",
    "- Visualizar la frontera de decisión y los márgenes\n",
    "- Identificar los vectores de soporte"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Importación de Bibliotecas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn import svm\n",
    "from sklearn.datasets import make_blobs\n",
    "from sklearn.inspection import DecisionBoundaryDisplay\n",
    "\n",
    "# Configuración para mejorar la visualización\n",
    "plt.rcParams['figure.figsize'] = (10, 8)\n",
    "plt.rcParams['font.size'] = 11"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Generación de Datos Sintéticos\n",
    "\n",
    "Utilizamos `make_blobs` para crear un conjunto de datos con dos clases bien separadas. Este generador crea grupos (blobs) de puntos con distribución gaussiana.\n",
    "\n",
    "**Parámetros utilizados:**\n",
    "- `n_samples=40`: Total de 40 muestras\n",
    "- `centers=2`: Dos centros (dos clases)\n",
    "- `random_state=6`: Semilla para reproducibilidad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generamos 40 puntos separables en dos clases\n",
    "X, y = make_blobs(n_samples=40, centers=2, random_state=6)\n",
    "\n",
    "print(\"Forma del conjunto de características (X):\", X.shape)\n",
    "print(\"Forma del vector de etiquetas (y):\", y.shape)\n",
    "print(\"\\nPrimeras 5 muestras:\")\n",
    "print(\"X:\\n\", X[:5])\n",
    "print(\"y:\", y[:5])\n",
    "\n",
    "# Estadísticas básicas\n",
    "print(f\"\\nClase 0: {np.sum(y == 0)} muestras\")\n",
    "print(f\"Clase 1: {np.sum(y == 1)} muestras\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Entrenamiento del Modelo SVM\n",
    "\n",
    "Configuramos un clasificador SVM con las siguientes características:\n",
    "\n",
    "- **Kernel lineal**: Busca un hiperplano lineal de separación\n",
    "- **C=1000**: Parámetro de regularización muy alto, lo que significa que penalizamos fuertemente los errores de clasificación. Esto resulta en un margen más estrecho pero con menos errores en los datos de entrenamiento.\n",
    "\n",
    "### ¿Qué es C?\n",
    "El parámetro C controla el trade-off entre:\n",
    "- Maximizar el margen (distancia entre las clases)\n",
    "- Minimizar los errores de clasificación\n",
    "\n",
    "Un C alto → margen estrecho, menos errores de entrenamiento (puede sobreajustar)  \n",
    "Un C bajo → margen amplio, permite más errores (mayor generalización)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Entrenamos el modelo SVM\n",
    "clf = svm.SVC(kernel=\"linear\", C=1000)\n",
    "clf.fit(X, y)\n",
    "\n",
    "print(\"Modelo entrenado exitosamente\")\n",
    "print(f\"Número de vectores de soporte: {len(clf.support_vectors_)}\")\n",
    "print(f\"Índices de los vectores de soporte: {clf.support_}\")\n",
    "print(f\"Vectores de soporte por clase: {clf.n_support_}\")\n",
    "\n",
    "# Coeficientes del hiperplano\n",
    "print(f\"\\nCoeficientes del hiperplano (w): {clf.coef_}\")\n",
    "print(f\"Intercepto (b): {clf.intercept_}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Visualización de Resultados\n",
    "\n",
    "Creamos una visualización completa que muestra:\n",
    "\n",
    "1. **Puntos de datos**: Coloreados según su clase\n",
    "2. **Hiperplano de decisión**: Línea sólida (nivel 0)\n",
    "3. **Márgenes**: Líneas punteadas (niveles -1 y +1)\n",
    "4. **Vectores de soporte**: Puntos marcados con un círculo negro\n",
    "\n",
    "Los vectores de soporte son los puntos críticos que definen la posición del hiperplano de decisión. Son los puntos más cercanos al límite de decisión y son los únicos que influyen en la definición del modelo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crear la visualización\n",
    "plt.figure(figsize=(10, 8))\n",
    "\n",
    "# Graficar los puntos de datos\n",
    "plt.scatter(X[:, 0], X[:, 1], c=y, s=30, cmap=plt.cm.Paired, edgecolors='k')\n",
    "\n",
    "# Obtener el eje actual\n",
    "ax = plt.gca()\n",
    "\n",
    "# Graficar la frontera de decisión y los márgenes\n",
    "DecisionBoundaryDisplay.from_estimator(\n",
    "    clf,\n",
    "    X,\n",
    "    plot_method=\"contour\",\n",
    "    colors=\"k\",\n",
    "    levels=[-1, 0, 1],\n",
    "    alpha=0.5,\n",
    "    linestyles=[\"--\", \"-\", \"--\"],\n",
    "    ax=ax,\n",
    ")\n",
    "\n",
    "# Resaltar los vectores de soporte\n",
    "ax.scatter(\n",
    "    clf.support_vectors_[:, 0],\n",
    "    clf.support_vectors_[:, 1],\n",
    "    s=100,\n",
    "    linewidth=1,\n",
    "    facecolors=\"none\",\n",
    "    edgecolors=\"k\",\n",
    "    label=\"Vectores de soporte\"\n",
    ")\n",
    "\n",
    "plt.xlabel(\"Característica 1\")\n",
    "plt.ylabel(\"Característica 2\")\n",
    "plt.title(\"SVM con Kernel Lineal - Frontera de Decisión y Vectores de Soporte\")\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Análisis de los Vectores de Soporte\n",
    "\n",
    "Vamos a examinar más de cerca los vectores de soporte identificados por el modelo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Vectores de soporte:\")\n",
    "print(clf.support_vectors_)\n",
    "print(f\"\\nTotal de vectores de soporte: {len(clf.support_vectors_)}\")\n",
    "print(f\"Porcentaje de datos que son vectores de soporte: {len(clf.support_vectors_)/len(X)*100:.1f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Evaluación del Modelo\n",
    "\n",
    "Evaluamos el rendimiento del modelo en los datos de entrenamiento."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predicciones\n",
    "y_pred = clf.predict(X)\n",
    "\n",
    "# Precisión\n",
    "accuracy = clf.score(X, y)\n",
    "print(f\"Precisión en entrenamiento: {accuracy*100:.2f}%\")\n",
    "\n",
    "# Matriz de confusión manual\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "\n",
    "cm = confusion_matrix(y, y_pred)\n",
    "print(\"\\nMatriz de confusión:\")\n",
    "print(cm)\n",
    "\n",
    "print(\"\\nReporte de clasificación:\")\n",
    "print(classification_report(y, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Conclusiones\n",
    "\n",
    "En este notebook hemos:\n",
    "\n",
    "1. ✅ Generado un conjunto de datos sintético linealmente separable con 40 muestras\n",
    "2. ✅ Entrenado un clasificador SVM con kernel lineal y parámetro de regularización C=1000\n",
    "3. ✅ Visualizado la frontera de decisión, los márgenes y los vectores de soporte\n",
    "4. ✅ Analizado el desempeño del modelo\n",
    "\n",
    "### Observaciones clave:\n",
    "\n",
    "- Los **vectores de soporte** son los puntos críticos que definen el hiperplano de separación\n",
    "- Un valor alto de **C** resulta en un margen más estrecho pero con mejor ajuste a los datos de entrenamiento\n",
    "- El modelo logra separar perfectamente las dos clases en este caso linealmente separable\n",
    "\n",
    "### Experimentos sugeridos:\n",
    "\n",
    "- Cambiar el valor de C y observar cómo afecta el número de vectores de soporte\n",
    "- Modificar `random_state` para generar diferentes distribuciones de datos\n",
    "- Probar con más muestras o con datos que no sean linealmente separables"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
