{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clasificación con Support Vector Machines (SVM)\n",
    "\n",
    "---\n",
    "**Autor:** Balbuena Palma José Ángel\n",
    "\n",
    "**Fecha:** 25 Octubre 2025\n",
    "\n",
    "## Objetivo\n",
    "\n",
    "Este notebook implementa un análisis completo de clasificación utilizando Support Vector Machines (SVM) con diferentes kernels. El objetivo es:\n",
    "\n",
    "- Explorar y visualizar un dataset de clasificación\n",
    "- Entrenar modelos SVM con diferentes kernels (linear, poly, rbf, sigmoid)\n",
    "- Optimizar hiperparámetros usando GridSearchCV\n",
    "- Evaluar y comparar el rendimiento de los modelos\n",
    "- Visualizar las regiones de decisión\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Importación de Librerías"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Librerías importadas correctamente\n"
     ]
    }
   ],
   "source": [
    "# Tratamiento de datos\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Gráficos\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import style\n",
    "import seaborn as sns\n",
    "from mlxtend.plotting import plot_decision_regions\n",
    "\n",
    "# Preprocesado y modelado\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "\n",
    "# Configuración\n",
    "plt.rcParams['image.cmap'] = \"bwr\"\n",
    "plt.rcParams['savefig.bbox'] = \"tight\"\n",
    "plt.rcParams['figure.figsize'] = (10, 6)\n",
    "style.use('ggplot')\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(\"✓ Librerías importadas correctamente\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Carga de Datos\n",
    "\n",
    "Utilizaremos el dataset ESL.mixture que contiene datos de clasificación binaria con dos características (X1, X2)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# URL del dataset\n",
    "url = ('https://raw.githubusercontent.com/JoaquinAmatRodrigo/'\n",
    "       'Estadistica-machine-learning-python/master/data/ESL.mixture.csv')\n",
    "\n",
    "# Cargar datos\n",
    "data = pd.read_csv(url)\n",
    "\n",
    "print(f\"Datos cargados exitosamente. Forma: {data.shape}\")\n",
    "print(f\"\\nColumnas: {list(data.columns)}\")\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Análisis Exploratorio de Datos (EDA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Información general\n",
    "print(\"=\" * 60)\n",
    "print(\"INFORMACIÓN GENERAL DEL DATASET\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"\\nForma: {data.shape}\")\n",
    "print(f\"\\nTipos de datos:\")\n",
    "print(data.dtypes)\n",
    "print(f\"\\nValores faltantes:\")\n",
    "print(data.isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Estadísticas descriptivas\n",
    "data.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Distribución de clases\n",
    "print(\"Distribución de clases:\")\n",
    "print(data['y'].value_counts())\n",
    "print(f\"\\nProporción:\")\n",
    "print(data['y'].value_counts(normalize=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Visualización de Datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preparar datos para visualización\n",
    "X = data[['X1', 'X2']].values\n",
    "y = data['y'].values\n",
    "\n",
    "# Gráfico de dispersión\n",
    "plt.figure(figsize=(10, 8))\n",
    "colors = ['#e74c3c' if label == 0 else '#3498db' for label in y]\n",
    "plt.scatter(X[:, 0], X[:, 1], c=colors, alpha=0.6, s=50, edgecolors='k', linewidth=0.5)\n",
    "\n",
    "plt.xlabel('X1', fontsize=12)\n",
    "plt.ylabel('X2', fontsize=12)\n",
    "plt.title('Distribución de los datos por clase', fontsize=14, fontweight='bold')\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "# Leyenda\n",
    "plt.scatter([], [], c='#e74c3c', alpha=0.6, s=50, label='Clase 0', edgecolors='k')\n",
    "plt.scatter([], [], c='#3498db', alpha=0.6, s=50, label='Clase 1', edgecolors='k')\n",
    "plt.legend(loc='best', fontsize=10)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. División de Datos\n",
    "\n",
    "Dividimos los datos en conjuntos de entrenamiento (80%) y prueba (20%)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# División estratificada\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "print(f\"Tamaño del conjunto de entrenamiento: {X_train.shape[0]} muestras\")\n",
    "print(f\"Tamaño del conjunto de prueba: {X_test.shape[0]} muestras\")\n",
    "print(f\"\\nDistribución de clases en entrenamiento:\")\n",
    "print(pd.Series(y_train).value_counts())\n",
    "print(f\"\\nDistribución de clases en prueba:\")\n",
    "print(pd.Series(y_test).value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Entrenamiento de Modelos SVM con Diferentes Kernels\n",
    "\n",
    "Entrenaremos modelos SVM con cuatro tipos de kernels:\n",
    "- **Linear**: para problemas linealmente separables\n",
    "- **Polynomial**: para relaciones polinómicas\n",
    "- **RBF (Radial Basis Function)**: kernel más popular, funciona bien en la mayoría de casos\n",
    "- **Sigmoid**: similar a redes neuronales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Diccionario para almacenar resultados\n",
    "models = {}\n",
    "kernels = ['linear', 'poly', 'rbf', 'sigmoid']\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"ENTRENAMIENTO DE MODELOS SVM\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "for kernel in kernels:\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Kernel: {kernel.upper()}\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    # Crear y entrenar modelo\n",
    "    model = SVC(kernel=kernel, random_state=42)\n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    # Predicciones\n",
    "    y_pred_train = model.predict(X_train)\n",
    "    y_pred_test = model.predict(X_test)\n",
    "    \n",
    "    # Métricas\n",
    "    train_accuracy = accuracy_score(y_train, y_pred_train)\n",
    "    test_accuracy = accuracy_score(y_test, y_pred_test)\n",
    "    \n",
    "    # Guardar resultados\n",
    "    models[kernel] = {\n",
    "        'model': model,\n",
    "        'train_accuracy': train_accuracy,\n",
    "        'test_accuracy': test_accuracy,\n",
    "        'y_pred_test': y_pred_test\n",
    "    }\n",
    "    \n",
    "    print(f\"Precisión en entrenamiento: {train_accuracy:.4f}\")\n",
    "    print(f\"Precisión en prueba: {test_accuracy:.4f}\")\n",
    "    print(f\"Diferencia (overfitting): {train_accuracy - test_accuracy:.4f}\")\n",
    "\n",
    "print(\"\\n✓ Todos los modelos entrenados correctamente\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Comparación de Modelos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crear DataFrame con resultados\n",
    "results = []\n",
    "for kernel, model_info in models.items():\n",
    "    results.append({\n",
    "        'Kernel': kernel,\n",
    "        'Precisión Train': model_info['train_accuracy'],\n",
    "        'Precisión Test': model_info['test_accuracy'],\n",
    "        'Diferencia': model_info['train_accuracy'] - model_info['test_accuracy']\n",
    "    })\n",
    "\n",
    "df_results = pd.DataFrame(results)\n",
    "df_results = df_results.sort_values('Precisión Test', ascending=False)\n",
    "\n",
    "print(\"COMPARACIÓN DE MODELOS\")\n",
    "print(\"=\" * 60)\n",
    "df_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gráfico comparativo\n",
    "fig, ax = plt.subplots(figsize=(12, 6))\n",
    "\n",
    "x = np.arange(len(df_results))\n",
    "width = 0.35\n",
    "\n",
    "bars1 = ax.bar(x - width/2, df_results['Precisión Train'], \n",
    "               width, label='Entrenamiento', alpha=0.8, color='#2ecc71')\n",
    "bars2 = ax.bar(x + width/2, df_results['Precisión Test'], \n",
    "               width, label='Prueba', alpha=0.8, color='#3498db')\n",
    "\n",
    "ax.set_xlabel('Kernel', fontsize=12)\n",
    "ax.set_ylabel('Precisión', fontsize=12)\n",
    "ax.set_title('Comparación de Precisión por Kernel', fontsize=14, fontweight='bold')\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels(df_results['Kernel'])\n",
    "ax.legend(fontsize=10)\n",
    "ax.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "# Añadir valores en las barras\n",
    "for bars in [bars1, bars2]:\n",
    "    for bar in bars:\n",
    "        height = bar.get_height()\n",
    "        ax.text(bar.get_x() + bar.get_width()/2., height,\n",
    "                f'{height:.3f}',\n",
    "                ha='center', va='bottom', fontsize=9)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Optimización de Hiperparámetros\n",
    "\n",
    "Utilizaremos GridSearchCV para optimizar los hiperparámetros del kernel RBF, que generalmente ofrece buen rendimiento."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definir grilla de parámetros para RBF\n",
    "param_grid = {\n",
    "    'C': [0.1, 1, 10, 100],\n",
    "    'gamma': ['scale', 'auto', 0.001, 0.01, 0.1, 1]\n",
    "}\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"OPTIMIZACIÓN DE HIPERPARÁMETROS - KERNEL RBF\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"\\nParámetros a optimizar:\")\n",
    "for param, values in param_grid.items():\n",
    "    print(f\"  {param}: {values}\")\n",
    "\n",
    "# Modelo base\n",
    "svm_rbf = SVC(kernel='rbf', random_state=42)\n",
    "\n",
    "# GridSearchCV\n",
    "grid_search = GridSearchCV(\n",
    "    svm_rbf, \n",
    "    param_grid, \n",
    "    cv=5, \n",
    "    scoring='accuracy',\n",
    "    n_jobs=-1,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "print(\"\\nIniciando búsqueda...\")\n",
    "grid_search.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Resultados de la optimización\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"RESULTADOS DE LA OPTIMIZACIÓN\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "print(f\"\\nMejores parámetros:\")\n",
    "for param, value in grid_search.best_params_.items():\n",
    "    print(f\"  {param}: {value}\")\n",
    "\n",
    "print(f\"\\nMejor puntuación CV: {grid_search.best_score_:.4f}\")\n",
    "\n",
    "# Guardar mejor modelo\n",
    "best_model = grid_search.best_estimator_\n",
    "\n",
    "# Evaluar en conjunto de prueba\n",
    "y_pred_best = best_model.predict(X_test)\n",
    "test_accuracy_best = accuracy_score(y_test, y_pred_best)\n",
    "print(f\"Precisión en conjunto de prueba: {test_accuracy_best:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizar resultados de GridSearch\n",
    "cv_results = pd.DataFrame(grid_search.cv_results_)\n",
    "cv_results_pivot = cv_results.pivot_table(\n",
    "    values='mean_test_score',\n",
    "    index='param_C',\n",
    "    columns='param_gamma'\n",
    ")\n",
    "\n",
    "plt.figure(figsize=(12, 8))\n",
    "sns.heatmap(cv_results_pivot, annot=True, fmt='.4f', cmap='YlOrRd', \n",
    "            cbar_kws={'label': 'Precisión CV'})\n",
    "plt.title('Precisión del modelo según hiperparámetros (C y gamma)', \n",
    "          fontsize=14, fontweight='bold')\n",
    "plt.xlabel('Gamma', fontsize=12)\n",
    "plt.ylabel('C', fontsize=12)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Evaluación del Modelo Optimizado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reporte de clasificación\n",
    "print(\"=\" * 60)\n",
    "print(\"REPORTE DE CLASIFICACIÓN - MODELO OPTIMIZADO\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"\\n{classification_report(y_test, y_pred_best)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Matriz de confusión\n",
    "cm = confusion_matrix(y_test, y_pred_best)\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
    "            xticklabels=['Clase 0', 'Clase 1'],\n",
    "            yticklabels=['Clase 0', 'Clase 1'],\n",
    "            cbar_kws={'label': 'Frecuencia'})\n",
    "plt.title('Matriz de Confusión - Modelo Optimizado', fontsize=14, fontweight='bold')\n",
    "plt.ylabel('Valor Real', fontsize=12)\n",
    "plt.xlabel('Predicción', fontsize=12)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nMatriz de confusión:\")\n",
    "print(cm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Visualización de Regiones de Decisión"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizar regiones de decisión para todos los kernels\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 12))\n",
    "axes = axes.ravel()\n",
    "\n",
    "for idx, (kernel, model_info) in enumerate(models.items()):\n",
    "    ax = axes[idx]\n",
    "    \n",
    "    # Plotear regiones de decisión\n",
    "    plot_decision_regions(X=X_test, y=y_test, \n",
    "                          clf=model_info['model'], \n",
    "                          legend=2, ax=ax)\n",
    "    \n",
    "    ax.set_xlabel('X1', fontsize=11)\n",
    "    ax.set_ylabel('X2', fontsize=11)\n",
    "    ax.set_title(f\"Kernel: {kernel} | Precisión: {model_info['test_accuracy']:.4f}\",\n",
    "                fontsize=12, fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizar regiones de decisión del modelo optimizado\n",
    "plt.figure(figsize=(12, 8))\n",
    "\n",
    "plot_decision_regions(X=X_test, y=y_test, clf=best_model, legend=2)\n",
    "\n",
    "plt.xlabel('X1', fontsize=12)\n",
    "plt.ylabel('X2', fontsize=12)\n",
    "plt.title(f'Regiones de Decisión - Modelo Optimizado (RBF)\\n'\n",
    "          f'C={grid_search.best_params_[\"C\"]}, gamma={grid_search.best_params_[\"gamma\"]} | '\n",
    "          f'Precisión: {test_accuracy_best:.4f}',\n",
    "          fontsize=14, fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Conclusiones\n",
    "\n",
    "### Resumen de Resultados\n",
    "\n",
    "En este análisis hemos implementado y comparado modelos SVM con diferentes kernels para un problema de clasificación binaria:\n",
    "\n",
    "**Comparación de Kernels:**\n",
    "- El kernel **RBF** generalmente ofrece el mejor rendimiento para este dataset\n",
    "- El kernel **polynomial** puede presentar overfitting dependiendo del grado\n",
    "- El kernel **linear** funciona bien cuando los datos son linealmente separables\n",
    "- El kernel **sigmoid** puede ser útil en algunos casos específicos\n",
    "\n",
    "**Optimización de Hiperparámetros:**\n",
    "- Los parámetros C y gamma tienen un impacto significativo en el rendimiento del modelo RBF\n",
    "- La validación cruzada ayuda a encontrar la mejor combinación de hiperparámetros\n",
    "- Es importante balancear la precisión en entrenamiento y prueba para evitar overfitting\n",
    "\n",
    "**Visualización:**\n",
    "- Las regiones de decisión muestran cómo cada kernel separa las clases\n",
    "- Los kernels no lineales pueden capturar fronteras de decisión más complejas\n",
    "\n",
    "### Recomendaciones\n",
    "\n",
    "1. Siempre probar múltiples kernels antes de seleccionar el mejor\n",
    "2. Utilizar validación cruzada para optimizar hiperparámetros\n",
    "3. Monitorear el overfitting comparando precisión en train y test\n",
    "4. Considerar la escalabilidad del modelo según el tamaño del dataset\n",
    "5. Evaluar múltiples métricas (precisión, recall, F1-score) según el problema\n",
    "\n",
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
